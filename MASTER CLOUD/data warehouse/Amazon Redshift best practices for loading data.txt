Take the loading data tutorial
Use a COPY command to load data
Use a single COPY command to load from multiple files
Split your load data
Verify data files before and after a load
Load data in sort key order
Load data in sequential blocks
Use time-series tables
Use a staging table to perform a merge (upsert)
Schedule around maintenance windows



1. Use a bulk insert :
        create table category_stage as
        select * from category;

        insert into category_stage
        (select * from category);



2. Use a multi-row insert :
        If a COPY command is not an option and you require SQL inserts, use a multi-row insert
        Data compression is inefficient when you add data only one row or a few rows at a time.
        Multi-row inserts improve performance by batching up a series of inserts.


        insert into category_stage values
        (default, default, default, default),
        (20, default, 'Country', default),
        (21, 'Concerts', 'Rock', default);

3. Compress your data files
        When you want to compress large load files, we recommend that you use gzip, lzop, bzip2, or
        Zstandard to compress them and split the data into multiple smaller files.

        copy time
        from 's3://mybucket/data/timerows.lzo'
        iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'
        lzop
        delimiter '|';




